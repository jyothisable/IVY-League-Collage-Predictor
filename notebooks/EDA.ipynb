{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Intro to the Dataset and the Aim\n",
    "\\<img src=\"/jamboree_logo.png\" alt=\"jamboree logo banner\" style=\"width: 800px;\"/>\n",
    "\n",
    "Jamboree has helped thousands of students like you make it to top colleges abroad. Be it GMAT, GRE or SAT, their unique problem-solving methods ensure maximum scores with minimum effort.\n",
    "\n",
    "Jamboree team wants to know what factors are important for a students success in getting into an IVY league college. They also want to see if we can make a predictive model to predict the chance of admission to IVY league college using the given features.\n",
    "\n",
    "**Dataset**\n",
    "\n",
    "This dataset contains the details of 500 students who have applied for admission to IVY league college along with their success rate.\n",
    "\n",
    "Summary of sanitized data:\n",
    "| Column              | Description         | Data Type  |\n",
    "|---------------------|---------------------|------------|\n",
    "| `serial_no`         | Unique row ID       | `int64`    |\n",
    "| `gre_score`         | out of 340          | `int64`    |\n",
    "| `toefl_score`       | out of 120          | `int64`    |\n",
    "| `university_rating` | out of 5            | `category` |\n",
    "| `sop`               | out of 5            | `category` |\n",
    "| `lor`               | out of 5            | `category` |\n",
    "| `cgpa`              | out of 10           | `category` |\n",
    "| `research`          | either 0 or 1       | `category` |\n",
    "| `chance_of_admit`   | ranging from 0 to 1 | `float64`  |\n",
    "\n",
    "Additional feature engineered columns:\n",
    "\n",
    "| Column | Description | Expected Data Type |\n",
    "|--------|-------------|--------------------|\n",
    "| `GRE`  | out of 340  | `int64`            |\n",
    "\n",
    "**Aim:** \n",
    "1. To anlyze what factors are important for a students success in getting into an IVY league college.\n",
    "2. To make a predictive model to predict the chance of admission (`chance_of_admit`) to IVY league college using the given features.\n",
    "\n",
    "**Methods and Techniques used:** EDA, feature engineering, modeling using sklearn pipelines, hyperparameter tuning\n",
    "\n",
    "**Measure of Performance and Minimum Threshold to reach the business objective** : RMSE of 5% or less\n",
    "\n",
    "**Assumptions**\n",
    "1. This fairly small dataset (500 entries) is representative of the real world population.\n",
    "2. The data is stable and does not change over time. Thus model assumed to not decay. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Library Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scientific libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Logging\n",
    "import logging\n",
    "\n",
    "# Visual libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Helper libraries\n",
    "import urllib.request\n",
    "from tqdm.notebook import tqdm, trange # Progress bar\n",
    "#from colorama import Fore, Back, Style # coloured text in output\n",
    "import warnings \n",
    "#warnings.filterwarnings('ignore') # ignore all warkings\n",
    "\n",
    "# Visual setup\n",
    "%config InlineBackend.figure_format = 'retina' # sets the figure format to 'retina' for high-resolution displays.\n",
    "\n",
    "# Pandas options\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "# InteractiveShell.ast_node_interactivity = 'all' # display all interaction \n",
    "\n",
    "# Table styles\n",
    "table_styles = {\n",
    "    'cerulean_palette': [\n",
    "        dict(selector=\"th\", props=[(\"color\", \"#FFFFFF\"), (\"background\", \"#004D80\")]),\n",
    "        dict(selector=\"td\", props=[(\"color\", \"#333333\")]),\n",
    "        dict(selector=\"table\", props=[(\"font-family\", 'Arial'), (\"border-collapse\", \"collapse\")]),\n",
    "        dict(selector='tr:nth-child(even)', props=[('background', '#D3EEFF')]),\n",
    "        dict(selector='tr:nth-child(odd)', props=[('background', '#FFFFFF')]),\n",
    "        dict(selector=\"th\", props=[(\"border\", \"1px solid #0070BA\")]),\n",
    "        dict(selector=\"td\", props=[(\"border\", \"1px solid #0070BA\")]),\n",
    "        dict(selector=\"tr:hover\", props=[(\"background\", \"#80D0FF\")]),\n",
    "        dict(selector=\"tr\", props=[(\"transition\", \"background 0.5s ease\")]),\n",
    "        dict(selector=\"th:hover\", props=[(\"font-size\", \"1.07rem\")]),\n",
    "        dict(selector=\"th\", props=[(\"transition\", \"font-size 0.5s ease-in-out\")]),\n",
    "        dict(selector=\"td:hover\", props=[('font-size', '1.07rem'),('font-weight', 'bold')]),\n",
    "        dict(selector=\"td\", props=[(\"transition\", \"font-size 0.5s ease-in-out\")])\n",
    "    ]\n",
    "}\n",
    "\n",
    "#from rich import print # color from print statement \n",
    "# Seed value for numpy.random => makes notebooks stable across runs\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Collection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataIngestor:\n",
    "    \"\"\"\"\n",
    "    A class to handle downloading data and loading it into a pandas dataframe along with basic sanity options\n",
    "    \"\"\"\n",
    "    def __init__(self, file_path : str = '../data/raw', url : str = None, output_path : str = '../data/processed/processed.csv'):\n",
    "        if (url is None and file_path == 'data/raw') or (url is not None and file_path != '../data/raw'):\n",
    "            raise ValueError('Either url or file_path must/only be specified')\n",
    "        self.file_path = f\"{file_path}\"+f\"/{url.split('/')[-1]}\" if url is not None else file_path # save non default user specified path\n",
    "        self.url = url\n",
    "        self.output_path = output_path\n",
    "    \n",
    "    def download_data(self) -> None:\n",
    "        logging.info(f'Downloading data from {self.url}')\n",
    "        urllib.request.urlretrieve(self.url, self.file_path)\n",
    "\n",
    "    def load_data(self) -> pd.DataFrame:\n",
    "        logging.info(f'Ingesting data from {self.file_path}')\n",
    "        #TODO add csv check\n",
    "        return pd.read_csv(self.file_path)\n",
    "    \n",
    "    def save_data(self, df : pd.DataFrame) -> None:\n",
    "        logging.info(f'Saving data to {self.output_path}')\n",
    "        df.to_csv(self.output_path)\n",
    "        \n",
    "    def sanitize(self, df : pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Rename columns to snake case and strip whitespace\n",
    "        \"\"\"\n",
    "        return df.rename(lambda x: x.lower().strip().replace(' ', '_'),axis='columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_url = DataIngestor(url = 'https://d2beiqkhq929f0.cloudfront.net/public_assets/assets/000/001/839/original/Jamboree_Admission.csv')\n",
    "\n",
    "data_url.download_data()\n",
    "df = data_url.sanitize(data_url.load_data())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>serial_no.</th>\n",
       "      <th>gre_score</th>\n",
       "      <th>toefl_score</th>\n",
       "      <th>university_rating</th>\n",
       "      <th>sop</th>\n",
       "      <th>lor</th>\n",
       "      <th>cgpa</th>\n",
       "      <th>research</th>\n",
       "      <th>chance_of_admit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>337</td>\n",
       "      <td>118</td>\n",
       "      <td>4</td>\n",
       "      <td>4.5</td>\n",
       "      <td>4.5</td>\n",
       "      <td>9.65</td>\n",
       "      <td>1</td>\n",
       "      <td>0.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>324</td>\n",
       "      <td>107</td>\n",
       "      <td>4</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>8.87</td>\n",
       "      <td>1</td>\n",
       "      <td>0.76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>316</td>\n",
       "      <td>104</td>\n",
       "      <td>3</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>8.00</td>\n",
       "      <td>1</td>\n",
       "      <td>0.72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>322</td>\n",
       "      <td>110</td>\n",
       "      <td>3</td>\n",
       "      <td>3.5</td>\n",
       "      <td>2.5</td>\n",
       "      <td>8.67</td>\n",
       "      <td>1</td>\n",
       "      <td>0.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>314</td>\n",
       "      <td>103</td>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>8.21</td>\n",
       "      <td>0</td>\n",
       "      <td>0.65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>495</th>\n",
       "      <td>496</td>\n",
       "      <td>332</td>\n",
       "      <td>108</td>\n",
       "      <td>5</td>\n",
       "      <td>4.5</td>\n",
       "      <td>4.0</td>\n",
       "      <td>9.02</td>\n",
       "      <td>1</td>\n",
       "      <td>0.87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>496</th>\n",
       "      <td>497</td>\n",
       "      <td>337</td>\n",
       "      <td>117</td>\n",
       "      <td>5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>9.87</td>\n",
       "      <td>1</td>\n",
       "      <td>0.96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497</th>\n",
       "      <td>498</td>\n",
       "      <td>330</td>\n",
       "      <td>120</td>\n",
       "      <td>5</td>\n",
       "      <td>4.5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>9.56</td>\n",
       "      <td>1</td>\n",
       "      <td>0.93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498</th>\n",
       "      <td>499</td>\n",
       "      <td>312</td>\n",
       "      <td>103</td>\n",
       "      <td>4</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>8.43</td>\n",
       "      <td>0</td>\n",
       "      <td>0.73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499</th>\n",
       "      <td>500</td>\n",
       "      <td>327</td>\n",
       "      <td>113</td>\n",
       "      <td>4</td>\n",
       "      <td>4.5</td>\n",
       "      <td>4.5</td>\n",
       "      <td>9.04</td>\n",
       "      <td>0</td>\n",
       "      <td>0.84</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>500 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     serial_no.  gre_score  toefl_score  university_rating  sop  lor  cgpa  \\\n",
       "0             1        337          118                  4  4.5  4.5  9.65   \n",
       "1             2        324          107                  4  4.0  4.5  8.87   \n",
       "2             3        316          104                  3  3.0  3.5  8.00   \n",
       "3             4        322          110                  3  3.5  2.5  8.67   \n",
       "4             5        314          103                  2  2.0  3.0  8.21   \n",
       "..          ...        ...          ...                ...  ...  ...   ...   \n",
       "495         496        332          108                  5  4.5  4.0  9.02   \n",
       "496         497        337          117                  5  5.0  5.0  9.87   \n",
       "497         498        330          120                  5  4.5  5.0  9.56   \n",
       "498         499        312          103                  4  4.0  5.0  8.43   \n",
       "499         500        327          113                  4  4.5  4.5  9.04   \n",
       "\n",
       "     research  chance_of_admit  \n",
       "0           1             0.92  \n",
       "1           1             0.76  \n",
       "2           1             0.72  \n",
       "3           1             0.80  \n",
       "4           0             0.65  \n",
       "..        ...              ...  \n",
       "495         1             0.87  \n",
       "496         1             0.96  \n",
       "497         1             0.93  \n",
       "498         0             0.73  \n",
       "499         0             0.84  \n",
       "\n",
       "[500 rows x 9 columns]"
      ]
     },
     "execution_count": 419,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 500 entries, 0 to 499\n",
      "Data columns (total 9 columns):\n",
      " #   Column             Non-Null Count  Dtype  \n",
      "---  ------             --------------  -----  \n",
      " 0   serial_no.         500 non-null    int64  \n",
      " 1   gre_score          500 non-null    int64  \n",
      " 2   toefl_score        500 non-null    int64  \n",
      " 3   university_rating  500 non-null    int64  \n",
      " 4   sop                500 non-null    float64\n",
      " 5   lor                500 non-null    float64\n",
      " 6   cgpa               500 non-null    float64\n",
      " 7   research           500 non-null    int64  \n",
      " 8   chance_of_admit    500 non-null    float64\n",
      "dtypes: float64(4), int64(5)\n",
      "memory usage: 35.3 KB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>serial_no.</th>\n",
       "      <th>gre_score</th>\n",
       "      <th>toefl_score</th>\n",
       "      <th>university_rating</th>\n",
       "      <th>sop</th>\n",
       "      <th>lor</th>\n",
       "      <th>cgpa</th>\n",
       "      <th>research</th>\n",
       "      <th>chance_of_admit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>500.000000</td>\n",
       "      <td>500.000000</td>\n",
       "      <td>500.000000</td>\n",
       "      <td>500.000000</td>\n",
       "      <td>500.000000</td>\n",
       "      <td>500.00000</td>\n",
       "      <td>500.000000</td>\n",
       "      <td>500.000000</td>\n",
       "      <td>500.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>250.500000</td>\n",
       "      <td>316.472000</td>\n",
       "      <td>107.192000</td>\n",
       "      <td>3.114000</td>\n",
       "      <td>3.374000</td>\n",
       "      <td>3.48400</td>\n",
       "      <td>8.576440</td>\n",
       "      <td>0.560000</td>\n",
       "      <td>0.72174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>144.481833</td>\n",
       "      <td>11.295148</td>\n",
       "      <td>6.081868</td>\n",
       "      <td>1.143512</td>\n",
       "      <td>0.991004</td>\n",
       "      <td>0.92545</td>\n",
       "      <td>0.604813</td>\n",
       "      <td>0.496884</td>\n",
       "      <td>0.14114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>290.000000</td>\n",
       "      <td>92.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>6.800000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.34000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>125.750000</td>\n",
       "      <td>308.000000</td>\n",
       "      <td>103.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.500000</td>\n",
       "      <td>3.00000</td>\n",
       "      <td>8.127500</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.63000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>250.500000</td>\n",
       "      <td>317.000000</td>\n",
       "      <td>107.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.500000</td>\n",
       "      <td>3.50000</td>\n",
       "      <td>8.560000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.72000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>375.250000</td>\n",
       "      <td>325.000000</td>\n",
       "      <td>112.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.00000</td>\n",
       "      <td>9.040000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.82000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>500.000000</td>\n",
       "      <td>340.000000</td>\n",
       "      <td>120.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.00000</td>\n",
       "      <td>9.920000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.97000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       serial_no.   gre_score  toefl_score  university_rating         sop  \\\n",
       "count  500.000000  500.000000   500.000000         500.000000  500.000000   \n",
       "mean   250.500000  316.472000   107.192000           3.114000    3.374000   \n",
       "std    144.481833   11.295148     6.081868           1.143512    0.991004   \n",
       "min      1.000000  290.000000    92.000000           1.000000    1.000000   \n",
       "25%    125.750000  308.000000   103.000000           2.000000    2.500000   \n",
       "50%    250.500000  317.000000   107.000000           3.000000    3.500000   \n",
       "75%    375.250000  325.000000   112.000000           4.000000    4.000000   \n",
       "max    500.000000  340.000000   120.000000           5.000000    5.000000   \n",
       "\n",
       "             lor        cgpa    research  chance_of_admit  \n",
       "count  500.00000  500.000000  500.000000        500.00000  \n",
       "mean     3.48400    8.576440    0.560000          0.72174  \n",
       "std      0.92545    0.604813    0.496884          0.14114  \n",
       "min      1.00000    6.800000    0.000000          0.34000  \n",
       "25%      3.00000    8.127500    0.000000          0.63000  \n",
       "50%      3.50000    8.560000    1.000000          0.72000  \n",
       "75%      4.00000    9.040000    1.000000          0.82000  \n",
       "max      5.00000    9.920000    1.000000          0.97000  "
      ]
     },
     "execution_count": 419,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df\n",
    "df.info()\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Wrangling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data Integrity and Consistency**\n",
    "- [x] Uniformity Constraint: All data should be in the same unit or format like date, currency, scales.\n",
    "    - Example: Convert all date columns to a consistent format (e.g., 'YYYY-MM-DD').\n",
    "    -  Remove symbols like % or $ \n",
    "    -  Remove scaling like 10 means 10k rupees\n",
    "- [x] Data Type Constraint: Constrain each variable to a specific data type, and check for mixed data types within a column.\n",
    "    - Example: Convert a column with mixed integers and strings to a consistent data type.\n",
    "- [x] Data Range Constraints: Each variable could have a limited range like date < current date.\n",
    "    - Example: Check if all birth dates are before the current date.\n",
    "- [x] Uniqueness Constraint: Duplicate entries should not be there either complete row or multiple values for the same primary key => take groupby and average.\n",
    "    - Example: Check for duplicate customer IDs if there are multiple different income value / credit score then average it out \n",
    "- [x] Text Constraint: Text should be in the expected format for that variable like phone numbers and emails following patterns and fixed lengths.\n",
    "    - Example: Check if all email addresses follow the pattern `name@domain.com`.\n",
    "    - Removing special characters, trimming whitespaces, and handling inconsistent capitalizations.\n",
    "\t- Encoding Issues: Check for consistent encoding (e.g., UTF-8, ASCII) across text data.\n",
    "\t    - Example: Convert all text columns to UTF-8 encoding.\n",
    "- [x] Categorical Constraint and order: \n",
    "    - Uncollapsed categories with the same or similar names\n",
    "    - Rename and Specify order of category data types\n",
    "\n",
    "**Data Validation and Relationships**\n",
    "\n",
    "- [ ] Cross-Field Validation: Compare multiple variables to get cross-field validation like delivery date >= purchase date.\n",
    "    - Example: Check if the delivery date is always greater than or equal to the purchase date.\n",
    "\n",
    "- [ ] Referential Integrity: Validate foreign key relationships between tables or datasets.\n",
    "    - Example: Check if all order IDs in the orders table have a corresponding customer ID in the customers table.\n",
    "- [ ] Business Rules: Check for any specific business rules or domain-specific constraints that the data should adhere to.\n",
    "    - Example: In a retail dataset, check if the total order value is equal to the sum of item prices.\n",
    "- [ ] **Hierarchical Validation**: Validate the hierarchical relationships within the data, such as ensuring that a subcategory belongs to the correct main category.\n",
    "\t- Example: Check if all 'product_subcategory' values correctly correspond to their 'product_category' values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # -ve testing\n",
    "# df.iloc[0,1] = 4\n",
    "# df.iloc[2,2] = 4322\n",
    "# df.iloc[3:5] = 2423\n",
    "\n",
    "class DataWrangler:\n",
    "    \"\"\"\"\n",
    "    A class to handle cleaning and wrangling data\n",
    "    \"\"\"\n",
    "    def __init__(self, df : pd.DataFrame):\n",
    "        self.df = df\n",
    "        self.processed_df = pd.DataFrame()\n",
    "        \n",
    "        # Range constrain attributes\n",
    "        self.range_constrains = {}\n",
    "        self.failed_index_range_constrains = pd.Index([])\n",
    "        \n",
    "        # Data type constrain attributes\n",
    "        self.data_type_map = {}\n",
    "        \n",
    "        # Unique constrain attributes\n",
    "        self.unique_cols = []\n",
    "        self.failed_unique_cols = pd.Index([])\n",
    "        \n",
    "    \n",
    "    def set_data_type(self, data_type_map : dict) -> pd.DataFrame:\n",
    "        self.data_type_map = data_type_map\n",
    "        self.processed_df = self.df.astype(data_type_map)\n",
    "        return self.processed_df\n",
    "    \n",
    "    def check_range_constrain(self, constrains : dict) -> bool:\n",
    "        self.range_constrains = constrains\n",
    "        mask = np.array([False]*len(self.df))\n",
    "        for col,(min_range, max_range) in self.range_constrains.items(): \n",
    "            mask += (self.df[col] < min_range) | (self.df[col] > max_range)\n",
    "        self.failed_index_range_constrains = self.df[mask].index\n",
    "        if mask.any():\n",
    "            print('Range constrain failed for below rows:')\n",
    "            print(self.df.iloc[self.failed_index_range_constrains])\n",
    "            return False\n",
    "        else : return True\n",
    "    \n",
    "    def fix_range_constrain(self) -> pd.DataFrame:\n",
    "        logging.info(f'Removing entreis for fixing range constrain for {self.failed_index_range_constrains}')\n",
    "        self.processed_df = self.processed_df.drop(self.failed_index_range_constrains,errors='ignore')\n",
    "        return self.processed_df\n",
    "    \n",
    "    def check_unique(self, unique_cols : list) -> bool:\n",
    "        self.unique_cols = unique_cols\n",
    "        mask = np.array([False]*len(self.df))\n",
    "        for col in unique_cols:\n",
    "            mask += self.df[col].duplicated()\n",
    "        self.failed_unique_cols = self.df[mask].index\n",
    "        if mask.any():\n",
    "            print('Unique constrain failed for below rows:')\n",
    "            print(self.df.iloc[self.failed_unique_cols])\n",
    "            return False\n",
    "        else : return True\n",
    "    \n",
    "    def fix_unique(self) -> pd.DataFrame:\n",
    "        # TODO add an option to average the non categorical values based on unique columns using groupby \n",
    "        logging.info(f'Removing entreis for fixing unique constrain for {self.failed_unique_cols}')\n",
    "        self.processed_df = self.processed_df.drop(self.failed_unique_cols,errors='ignore')\n",
    "        return self.processed_df\n",
    "    \n",
    "    def set_categorical_order(self, order : dict) -> pd.DataFrame:\n",
    "        if self.data_type_map is None:\n",
    "            raise Exception('Please set data type first')\n",
    "        self.set_categorical_order = order\n",
    "        for col, order in self.set_categorical_order.items():\n",
    "            self.processed_df[col] = self.processed_df[col].cat.set_categories(order, ordered=True)\n",
    "        return self.processed_df \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>serial_no.</th>\n",
       "      <th>gre_score</th>\n",
       "      <th>toefl_score</th>\n",
       "      <th>university_rating</th>\n",
       "      <th>sop</th>\n",
       "      <th>lor</th>\n",
       "      <th>cgpa</th>\n",
       "      <th>research</th>\n",
       "      <th>chance_of_admit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>337</td>\n",
       "      <td>118</td>\n",
       "      <td>4</td>\n",
       "      <td>4.5</td>\n",
       "      <td>4.5</td>\n",
       "      <td>9.65</td>\n",
       "      <td>1</td>\n",
       "      <td>0.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>324</td>\n",
       "      <td>107</td>\n",
       "      <td>4</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>8.87</td>\n",
       "      <td>1</td>\n",
       "      <td>0.76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>316</td>\n",
       "      <td>104</td>\n",
       "      <td>3</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>8.00</td>\n",
       "      <td>1</td>\n",
       "      <td>0.72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>322</td>\n",
       "      <td>110</td>\n",
       "      <td>3</td>\n",
       "      <td>3.5</td>\n",
       "      <td>2.5</td>\n",
       "      <td>8.67</td>\n",
       "      <td>1</td>\n",
       "      <td>0.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>314</td>\n",
       "      <td>103</td>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>8.21</td>\n",
       "      <td>0</td>\n",
       "      <td>0.65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>495</th>\n",
       "      <td>496</td>\n",
       "      <td>332</td>\n",
       "      <td>108</td>\n",
       "      <td>5</td>\n",
       "      <td>4.5</td>\n",
       "      <td>4.0</td>\n",
       "      <td>9.02</td>\n",
       "      <td>1</td>\n",
       "      <td>0.87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>496</th>\n",
       "      <td>497</td>\n",
       "      <td>337</td>\n",
       "      <td>117</td>\n",
       "      <td>5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>9.87</td>\n",
       "      <td>1</td>\n",
       "      <td>0.96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497</th>\n",
       "      <td>498</td>\n",
       "      <td>330</td>\n",
       "      <td>120</td>\n",
       "      <td>5</td>\n",
       "      <td>4.5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>9.56</td>\n",
       "      <td>1</td>\n",
       "      <td>0.93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498</th>\n",
       "      <td>499</td>\n",
       "      <td>312</td>\n",
       "      <td>103</td>\n",
       "      <td>4</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>8.43</td>\n",
       "      <td>0</td>\n",
       "      <td>0.73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499</th>\n",
       "      <td>500</td>\n",
       "      <td>327</td>\n",
       "      <td>113</td>\n",
       "      <td>4</td>\n",
       "      <td>4.5</td>\n",
       "      <td>4.5</td>\n",
       "      <td>9.04</td>\n",
       "      <td>0</td>\n",
       "      <td>0.84</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>500 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     serial_no.  gre_score  toefl_score university_rating  sop  lor  cgpa  \\\n",
       "0             1        337          118                 4  4.5  4.5  9.65   \n",
       "1             2        324          107                 4  4.0  4.5  8.87   \n",
       "2             3        316          104                 3  3.0  3.5  8.00   \n",
       "3             4        322          110                 3  3.5  2.5  8.67   \n",
       "4             5        314          103                 2  2.0  3.0  8.21   \n",
       "..          ...        ...          ...               ...  ...  ...   ...   \n",
       "495         496        332          108                 5  4.5  4.0  9.02   \n",
       "496         497        337          117                 5  5.0  5.0  9.87   \n",
       "497         498        330          120                 5  4.5  5.0  9.56   \n",
       "498         499        312          103                 4  4.0  5.0  8.43   \n",
       "499         500        327          113                 4  4.5  4.5  9.04   \n",
       "\n",
       "    research  chance_of_admit  \n",
       "0          1             0.92  \n",
       "1          1             0.76  \n",
       "2          1             0.72  \n",
       "3          1             0.80  \n",
       "4          0             0.65  \n",
       "..       ...              ...  \n",
       "495        1             0.87  \n",
       "496        1             0.96  \n",
       "497        1             0.93  \n",
       "498        0             0.73  \n",
       "499        0             0.84  \n",
       "\n",
       "[500 rows x 9 columns]"
      ]
     },
     "execution_count": 421,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>serial_no.</th>\n",
       "      <th>gre_score</th>\n",
       "      <th>toefl_score</th>\n",
       "      <th>university_rating</th>\n",
       "      <th>sop</th>\n",
       "      <th>lor</th>\n",
       "      <th>cgpa</th>\n",
       "      <th>research</th>\n",
       "      <th>chance_of_admit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>337</td>\n",
       "      <td>118</td>\n",
       "      <td>4</td>\n",
       "      <td>4.5</td>\n",
       "      <td>4.5</td>\n",
       "      <td>9.65</td>\n",
       "      <td>1</td>\n",
       "      <td>0.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>324</td>\n",
       "      <td>107</td>\n",
       "      <td>4</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>8.87</td>\n",
       "      <td>1</td>\n",
       "      <td>0.76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>316</td>\n",
       "      <td>104</td>\n",
       "      <td>3</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>8.00</td>\n",
       "      <td>1</td>\n",
       "      <td>0.72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>322</td>\n",
       "      <td>110</td>\n",
       "      <td>3</td>\n",
       "      <td>3.5</td>\n",
       "      <td>2.5</td>\n",
       "      <td>8.67</td>\n",
       "      <td>1</td>\n",
       "      <td>0.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>314</td>\n",
       "      <td>103</td>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>8.21</td>\n",
       "      <td>0</td>\n",
       "      <td>0.65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>495</th>\n",
       "      <td>496</td>\n",
       "      <td>332</td>\n",
       "      <td>108</td>\n",
       "      <td>5</td>\n",
       "      <td>4.5</td>\n",
       "      <td>4.0</td>\n",
       "      <td>9.02</td>\n",
       "      <td>1</td>\n",
       "      <td>0.87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>496</th>\n",
       "      <td>497</td>\n",
       "      <td>337</td>\n",
       "      <td>117</td>\n",
       "      <td>5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>9.87</td>\n",
       "      <td>1</td>\n",
       "      <td>0.96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497</th>\n",
       "      <td>498</td>\n",
       "      <td>330</td>\n",
       "      <td>120</td>\n",
       "      <td>5</td>\n",
       "      <td>4.5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>9.56</td>\n",
       "      <td>1</td>\n",
       "      <td>0.93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498</th>\n",
       "      <td>499</td>\n",
       "      <td>312</td>\n",
       "      <td>103</td>\n",
       "      <td>4</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>8.43</td>\n",
       "      <td>0</td>\n",
       "      <td>0.73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499</th>\n",
       "      <td>500</td>\n",
       "      <td>327</td>\n",
       "      <td>113</td>\n",
       "      <td>4</td>\n",
       "      <td>4.5</td>\n",
       "      <td>4.5</td>\n",
       "      <td>9.04</td>\n",
       "      <td>0</td>\n",
       "      <td>0.84</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>500 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     serial_no.  gre_score  toefl_score university_rating  sop  lor  cgpa  \\\n",
       "0             1        337          118                 4  4.5  4.5  9.65   \n",
       "1             2        324          107                 4  4.0  4.5  8.87   \n",
       "2             3        316          104                 3  3.0  3.5  8.00   \n",
       "3             4        322          110                 3  3.5  2.5  8.67   \n",
       "4             5        314          103                 2  2.0  3.0  8.21   \n",
       "..          ...        ...          ...               ...  ...  ...   ...   \n",
       "495         496        332          108                 5  4.5  4.0  9.02   \n",
       "496         497        337          117                 5  5.0  5.0  9.87   \n",
       "497         498        330          120                 5  4.5  5.0  9.56   \n",
       "498         499        312          103                 4  4.0  5.0  8.43   \n",
       "499         500        327          113                 4  4.5  4.5  9.04   \n",
       "\n",
       "    research  chance_of_admit  \n",
       "0          1             0.92  \n",
       "1          1             0.76  \n",
       "2          1             0.72  \n",
       "3          1             0.80  \n",
       "4          0             0.65  \n",
       "..       ...              ...  \n",
       "495        1             0.87  \n",
       "496        1             0.96  \n",
       "497        1             0.93  \n",
       "498        0             0.73  \n",
       "499        0             0.84  \n",
       "\n",
       "[500 rows x 9 columns]"
      ]
     },
     "execution_count": 421,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "range_constrains = {\n",
    "    'gre_score': [0, 340],\n",
    "    'toefl_score': [0, 120],\n",
    "    'university_rating': [0, 5],\n",
    "    'sop': [0, 5],\n",
    "    'lor': [0, 5],\n",
    "    'cgpa': [0, 10],\n",
    "    'research': [0, 1],\n",
    "    'chance_of_admit': [0, 1]\n",
    "}\n",
    "\n",
    "data_type_map = {\n",
    "    'serial_no.': 'int32',\n",
    "    'gre_score': 'int32',\n",
    "    'toefl_score': 'int32',\n",
    "    'university_rating': 'category',\n",
    "    'sop': 'category',\n",
    "    'lor': 'category',\n",
    "    'cgpa': 'float32',\n",
    "    'research': 'category',\n",
    "    'chance_of_admit': 'float32'\n",
    "}\n",
    "\n",
    "unique_cols = ['serial_no.']\n",
    "\n",
    "categorical_order = {\n",
    "    'university_rating': [1, 2, 3, 4, 5],\n",
    "    'sop': [1, 1.5, 2, 2.5, 3, 3.5, 4, 4.5, 5],\n",
    "    'lor': [1, 1.5, 2, 2.5, 3, 3.5, 4, 4.5, 5],\n",
    "    'research': [0, 1]\n",
    "}\n",
    "\n",
    "clean_data = DataWrangler(df)\n",
    "clean_data.set_data_type(data_type_map)\n",
    "\n",
    "# Check range constrains\n",
    "if not clean_data.check_range_constrain(range_constrains):\n",
    "    clean_data.fix_range_constrain()\n",
    "\n",
    "# Check unique constrain\n",
    "if not clean_data.check_unique(unique_cols):\n",
    "    clean_data.fix_unique()\n",
    "\n",
    "# set categorical order\n",
    "clean_data.set_categorical_order(categorical_order)\n",
    "\n",
    "cleaned_df = clean_data.processed_df\n",
    "data_url.save_data(cleaned_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final data after wrangling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 500 entries, 0 to 499\n",
      "Data columns (total 9 columns):\n",
      " #   Column             Non-Null Count  Dtype   \n",
      "---  ------             --------------  -----   \n",
      " 0   serial_no.         500 non-null    int32   \n",
      " 1   gre_score          500 non-null    int32   \n",
      " 2   toefl_score        500 non-null    int32   \n",
      " 3   university_rating  500 non-null    category\n",
      " 4   sop                500 non-null    category\n",
      " 5   lor                500 non-null    category\n",
      " 6   cgpa               500 non-null    float32 \n",
      " 7   research           500 non-null    category\n",
      " 8   chance_of_admit    500 non-null    float32 \n",
      "dtypes: category(4), float32(2), int32(3)\n",
      "memory usage: 12.9 KB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_6efcb th {\n",
       "  color: #FFFFFF;\n",
       "  background: #004D80;\n",
       "}\n",
       "#T_6efcb td {\n",
       "  color: #333333;\n",
       "}\n",
       "#T_6efcb table {\n",
       "  font-family: Arial;\n",
       "  border-collapse: collapse;\n",
       "}\n",
       "#T_6efcb tr:nth-child(even) {\n",
       "  background: #D3EEFF;\n",
       "}\n",
       "#T_6efcb tr:nth-child(odd) {\n",
       "  background: #FFFFFF;\n",
       "}\n",
       "#T_6efcb th {\n",
       "  border: 1px solid #0070BA;\n",
       "}\n",
       "#T_6efcb td {\n",
       "  border: 1px solid #0070BA;\n",
       "}\n",
       "#T_6efcb tr:hover {\n",
       "  background: #80D0FF;\n",
       "}\n",
       "#T_6efcb tr {\n",
       "  transition: background 0.5s ease;\n",
       "}\n",
       "#T_6efcb th:hover {\n",
       "  font-size: 1.07rem;\n",
       "}\n",
       "#T_6efcb th {\n",
       "  transition: font-size 0.5s ease-in-out;\n",
       "}\n",
       "#T_6efcb td:hover {\n",
       "  font-size: 1.07rem;\n",
       "  font-weight: bold;\n",
       "}\n",
       "#T_6efcb td {\n",
       "  transition: font-size 0.5s ease-in-out;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_6efcb\">\n",
       "  <caption>Jamboree data</caption>\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_6efcb_level0_col0\" class=\"col_heading level0 col0\" >serial_no.</th>\n",
       "      <th id=\"T_6efcb_level0_col1\" class=\"col_heading level0 col1\" >gre_score</th>\n",
       "      <th id=\"T_6efcb_level0_col2\" class=\"col_heading level0 col2\" >toefl_score</th>\n",
       "      <th id=\"T_6efcb_level0_col3\" class=\"col_heading level0 col3\" >cgpa</th>\n",
       "      <th id=\"T_6efcb_level0_col4\" class=\"col_heading level0 col4\" >chance_of_admit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_6efcb_level0_row0\" class=\"row_heading level0 row0\" >count</th>\n",
       "      <td id=\"T_6efcb_row0_col0\" class=\"data row0 col0\" >500.000000</td>\n",
       "      <td id=\"T_6efcb_row0_col1\" class=\"data row0 col1\" >500.000000</td>\n",
       "      <td id=\"T_6efcb_row0_col2\" class=\"data row0 col2\" >500.000000</td>\n",
       "      <td id=\"T_6efcb_row0_col3\" class=\"data row0 col3\" >500.000000</td>\n",
       "      <td id=\"T_6efcb_row0_col4\" class=\"data row0 col4\" >500.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_6efcb_level0_row1\" class=\"row_heading level0 row1\" >mean</th>\n",
       "      <td id=\"T_6efcb_row1_col0\" class=\"data row1 col0\" >250.500000</td>\n",
       "      <td id=\"T_6efcb_row1_col1\" class=\"data row1 col1\" >316.472000</td>\n",
       "      <td id=\"T_6efcb_row1_col2\" class=\"data row1 col2\" >107.192000</td>\n",
       "      <td id=\"T_6efcb_row1_col3\" class=\"data row1 col3\" >8.576440</td>\n",
       "      <td id=\"T_6efcb_row1_col4\" class=\"data row1 col4\" >0.721740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_6efcb_level0_row2\" class=\"row_heading level0 row2\" >std</th>\n",
       "      <td id=\"T_6efcb_row2_col0\" class=\"data row2 col0\" >144.481833</td>\n",
       "      <td id=\"T_6efcb_row2_col1\" class=\"data row2 col1\" >11.295148</td>\n",
       "      <td id=\"T_6efcb_row2_col2\" class=\"data row2 col2\" >6.081868</td>\n",
       "      <td id=\"T_6efcb_row2_col3\" class=\"data row2 col3\" >0.604813</td>\n",
       "      <td id=\"T_6efcb_row2_col4\" class=\"data row2 col4\" >0.141140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_6efcb_level0_row3\" class=\"row_heading level0 row3\" >min</th>\n",
       "      <td id=\"T_6efcb_row3_col0\" class=\"data row3 col0\" >1.000000</td>\n",
       "      <td id=\"T_6efcb_row3_col1\" class=\"data row3 col1\" >290.000000</td>\n",
       "      <td id=\"T_6efcb_row3_col2\" class=\"data row3 col2\" >92.000000</td>\n",
       "      <td id=\"T_6efcb_row3_col3\" class=\"data row3 col3\" >6.800000</td>\n",
       "      <td id=\"T_6efcb_row3_col4\" class=\"data row3 col4\" >0.340000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_6efcb_level0_row4\" class=\"row_heading level0 row4\" >25%</th>\n",
       "      <td id=\"T_6efcb_row4_col0\" class=\"data row4 col0\" >125.750000</td>\n",
       "      <td id=\"T_6efcb_row4_col1\" class=\"data row4 col1\" >308.000000</td>\n",
       "      <td id=\"T_6efcb_row4_col2\" class=\"data row4 col2\" >103.000000</td>\n",
       "      <td id=\"T_6efcb_row4_col3\" class=\"data row4 col3\" >8.127500</td>\n",
       "      <td id=\"T_6efcb_row4_col4\" class=\"data row4 col4\" >0.630000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_6efcb_level0_row5\" class=\"row_heading level0 row5\" >50%</th>\n",
       "      <td id=\"T_6efcb_row5_col0\" class=\"data row5 col0\" >250.500000</td>\n",
       "      <td id=\"T_6efcb_row5_col1\" class=\"data row5 col1\" >317.000000</td>\n",
       "      <td id=\"T_6efcb_row5_col2\" class=\"data row5 col2\" >107.000000</td>\n",
       "      <td id=\"T_6efcb_row5_col3\" class=\"data row5 col3\" >8.560000</td>\n",
       "      <td id=\"T_6efcb_row5_col4\" class=\"data row5 col4\" >0.720000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_6efcb_level0_row6\" class=\"row_heading level0 row6\" >75%</th>\n",
       "      <td id=\"T_6efcb_row6_col0\" class=\"data row6 col0\" >375.250000</td>\n",
       "      <td id=\"T_6efcb_row6_col1\" class=\"data row6 col1\" >325.000000</td>\n",
       "      <td id=\"T_6efcb_row6_col2\" class=\"data row6 col2\" >112.000000</td>\n",
       "      <td id=\"T_6efcb_row6_col3\" class=\"data row6 col3\" >9.040000</td>\n",
       "      <td id=\"T_6efcb_row6_col4\" class=\"data row6 col4\" >0.820000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_6efcb_level0_row7\" class=\"row_heading level0 row7\" >max</th>\n",
       "      <td id=\"T_6efcb_row7_col0\" class=\"data row7 col0\" >500.000000</td>\n",
       "      <td id=\"T_6efcb_row7_col1\" class=\"data row7 col1\" >340.000000</td>\n",
       "      <td id=\"T_6efcb_row7_col2\" class=\"data row7 col2\" >120.000000</td>\n",
       "      <td id=\"T_6efcb_row7_col3\" class=\"data row7 col3\" >9.920000</td>\n",
       "      <td id=\"T_6efcb_row7_col4\" class=\"data row7 col4\" >0.970000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7f7c92e8a390>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_771b9 th {\n",
       "  color: #FFFFFF;\n",
       "  background: #004D80;\n",
       "}\n",
       "#T_771b9 td {\n",
       "  color: #333333;\n",
       "}\n",
       "#T_771b9 table {\n",
       "  font-family: Arial;\n",
       "  border-collapse: collapse;\n",
       "}\n",
       "#T_771b9 tr:nth-child(even) {\n",
       "  background: #D3EEFF;\n",
       "}\n",
       "#T_771b9 tr:nth-child(odd) {\n",
       "  background: #FFFFFF;\n",
       "}\n",
       "#T_771b9 th {\n",
       "  border: 1px solid #0070BA;\n",
       "}\n",
       "#T_771b9 td {\n",
       "  border: 1px solid #0070BA;\n",
       "}\n",
       "#T_771b9 tr:hover {\n",
       "  background: #80D0FF;\n",
       "}\n",
       "#T_771b9 tr {\n",
       "  transition: background 0.5s ease;\n",
       "}\n",
       "#T_771b9 th:hover {\n",
       "  font-size: 1.07rem;\n",
       "}\n",
       "#T_771b9 th {\n",
       "  transition: font-size 0.5s ease-in-out;\n",
       "}\n",
       "#T_771b9 td:hover {\n",
       "  font-size: 1.07rem;\n",
       "  font-weight: bold;\n",
       "}\n",
       "#T_771b9 td {\n",
       "  transition: font-size 0.5s ease-in-out;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_771b9\">\n",
       "  <caption>Jamboree data after cleaning</caption>\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_771b9_level0_col0\" class=\"col_heading level0 col0\" >serial_no.</th>\n",
       "      <th id=\"T_771b9_level0_col1\" class=\"col_heading level0 col1\" >gre_score</th>\n",
       "      <th id=\"T_771b9_level0_col2\" class=\"col_heading level0 col2\" >toefl_score</th>\n",
       "      <th id=\"T_771b9_level0_col3\" class=\"col_heading level0 col3\" >university_rating</th>\n",
       "      <th id=\"T_771b9_level0_col4\" class=\"col_heading level0 col4\" >sop</th>\n",
       "      <th id=\"T_771b9_level0_col5\" class=\"col_heading level0 col5\" >lor</th>\n",
       "      <th id=\"T_771b9_level0_col6\" class=\"col_heading level0 col6\" >cgpa</th>\n",
       "      <th id=\"T_771b9_level0_col7\" class=\"col_heading level0 col7\" >research</th>\n",
       "      <th id=\"T_771b9_level0_col8\" class=\"col_heading level0 col8\" >chance_of_admit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_771b9_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_771b9_row0_col0\" class=\"data row0 col0\" >1</td>\n",
       "      <td id=\"T_771b9_row0_col1\" class=\"data row0 col1\" >337</td>\n",
       "      <td id=\"T_771b9_row0_col2\" class=\"data row0 col2\" >118</td>\n",
       "      <td id=\"T_771b9_row0_col3\" class=\"data row0 col3\" >4</td>\n",
       "      <td id=\"T_771b9_row0_col4\" class=\"data row0 col4\" >4.500000</td>\n",
       "      <td id=\"T_771b9_row0_col5\" class=\"data row0 col5\" >4.500000</td>\n",
       "      <td id=\"T_771b9_row0_col6\" class=\"data row0 col6\" >9.650000</td>\n",
       "      <td id=\"T_771b9_row0_col7\" class=\"data row0 col7\" >1</td>\n",
       "      <td id=\"T_771b9_row0_col8\" class=\"data row0 col8\" >0.920000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_771b9_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_771b9_row1_col0\" class=\"data row1 col0\" >2</td>\n",
       "      <td id=\"T_771b9_row1_col1\" class=\"data row1 col1\" >324</td>\n",
       "      <td id=\"T_771b9_row1_col2\" class=\"data row1 col2\" >107</td>\n",
       "      <td id=\"T_771b9_row1_col3\" class=\"data row1 col3\" >4</td>\n",
       "      <td id=\"T_771b9_row1_col4\" class=\"data row1 col4\" >4.000000</td>\n",
       "      <td id=\"T_771b9_row1_col5\" class=\"data row1 col5\" >4.500000</td>\n",
       "      <td id=\"T_771b9_row1_col6\" class=\"data row1 col6\" >8.870000</td>\n",
       "      <td id=\"T_771b9_row1_col7\" class=\"data row1 col7\" >1</td>\n",
       "      <td id=\"T_771b9_row1_col8\" class=\"data row1 col8\" >0.760000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_771b9_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "      <td id=\"T_771b9_row2_col0\" class=\"data row2 col0\" >3</td>\n",
       "      <td id=\"T_771b9_row2_col1\" class=\"data row2 col1\" >316</td>\n",
       "      <td id=\"T_771b9_row2_col2\" class=\"data row2 col2\" >104</td>\n",
       "      <td id=\"T_771b9_row2_col3\" class=\"data row2 col3\" >3</td>\n",
       "      <td id=\"T_771b9_row2_col4\" class=\"data row2 col4\" >3.000000</td>\n",
       "      <td id=\"T_771b9_row2_col5\" class=\"data row2 col5\" >3.500000</td>\n",
       "      <td id=\"T_771b9_row2_col6\" class=\"data row2 col6\" >8.000000</td>\n",
       "      <td id=\"T_771b9_row2_col7\" class=\"data row2 col7\" >1</td>\n",
       "      <td id=\"T_771b9_row2_col8\" class=\"data row2 col8\" >0.720000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_771b9_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "      <td id=\"T_771b9_row3_col0\" class=\"data row3 col0\" >4</td>\n",
       "      <td id=\"T_771b9_row3_col1\" class=\"data row3 col1\" >322</td>\n",
       "      <td id=\"T_771b9_row3_col2\" class=\"data row3 col2\" >110</td>\n",
       "      <td id=\"T_771b9_row3_col3\" class=\"data row3 col3\" >3</td>\n",
       "      <td id=\"T_771b9_row3_col4\" class=\"data row3 col4\" >3.500000</td>\n",
       "      <td id=\"T_771b9_row3_col5\" class=\"data row3 col5\" >2.500000</td>\n",
       "      <td id=\"T_771b9_row3_col6\" class=\"data row3 col6\" >8.670000</td>\n",
       "      <td id=\"T_771b9_row3_col7\" class=\"data row3 col7\" >1</td>\n",
       "      <td id=\"T_771b9_row3_col8\" class=\"data row3 col8\" >0.800000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_771b9_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
       "      <td id=\"T_771b9_row4_col0\" class=\"data row4 col0\" >5</td>\n",
       "      <td id=\"T_771b9_row4_col1\" class=\"data row4 col1\" >314</td>\n",
       "      <td id=\"T_771b9_row4_col2\" class=\"data row4 col2\" >103</td>\n",
       "      <td id=\"T_771b9_row4_col3\" class=\"data row4 col3\" >2</td>\n",
       "      <td id=\"T_771b9_row4_col4\" class=\"data row4 col4\" >2.000000</td>\n",
       "      <td id=\"T_771b9_row4_col5\" class=\"data row4 col5\" >3.000000</td>\n",
       "      <td id=\"T_771b9_row4_col6\" class=\"data row4 col6\" >8.210000</td>\n",
       "      <td id=\"T_771b9_row4_col7\" class=\"data row4 col7\" >0</td>\n",
       "      <td id=\"T_771b9_row4_col8\" class=\"data row4 col8\" >0.650000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_771b9_level0_row5\" class=\"row_heading level0 row5\" >5</th>\n",
       "      <td id=\"T_771b9_row5_col0\" class=\"data row5 col0\" >6</td>\n",
       "      <td id=\"T_771b9_row5_col1\" class=\"data row5 col1\" >330</td>\n",
       "      <td id=\"T_771b9_row5_col2\" class=\"data row5 col2\" >115</td>\n",
       "      <td id=\"T_771b9_row5_col3\" class=\"data row5 col3\" >5</td>\n",
       "      <td id=\"T_771b9_row5_col4\" class=\"data row5 col4\" >4.500000</td>\n",
       "      <td id=\"T_771b9_row5_col5\" class=\"data row5 col5\" >3.000000</td>\n",
       "      <td id=\"T_771b9_row5_col6\" class=\"data row5 col6\" >9.340000</td>\n",
       "      <td id=\"T_771b9_row5_col7\" class=\"data row5 col7\" >1</td>\n",
       "      <td id=\"T_771b9_row5_col8\" class=\"data row5 col8\" >0.900000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_771b9_level0_row6\" class=\"row_heading level0 row6\" >6</th>\n",
       "      <td id=\"T_771b9_row6_col0\" class=\"data row6 col0\" >7</td>\n",
       "      <td id=\"T_771b9_row6_col1\" class=\"data row6 col1\" >321</td>\n",
       "      <td id=\"T_771b9_row6_col2\" class=\"data row6 col2\" >109</td>\n",
       "      <td id=\"T_771b9_row6_col3\" class=\"data row6 col3\" >3</td>\n",
       "      <td id=\"T_771b9_row6_col4\" class=\"data row6 col4\" >3.000000</td>\n",
       "      <td id=\"T_771b9_row6_col5\" class=\"data row6 col5\" >4.000000</td>\n",
       "      <td id=\"T_771b9_row6_col6\" class=\"data row6 col6\" >8.200000</td>\n",
       "      <td id=\"T_771b9_row6_col7\" class=\"data row6 col7\" >1</td>\n",
       "      <td id=\"T_771b9_row6_col8\" class=\"data row6 col8\" >0.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_771b9_level0_row7\" class=\"row_heading level0 row7\" >7</th>\n",
       "      <td id=\"T_771b9_row7_col0\" class=\"data row7 col0\" >8</td>\n",
       "      <td id=\"T_771b9_row7_col1\" class=\"data row7 col1\" >308</td>\n",
       "      <td id=\"T_771b9_row7_col2\" class=\"data row7 col2\" >101</td>\n",
       "      <td id=\"T_771b9_row7_col3\" class=\"data row7 col3\" >2</td>\n",
       "      <td id=\"T_771b9_row7_col4\" class=\"data row7 col4\" >3.000000</td>\n",
       "      <td id=\"T_771b9_row7_col5\" class=\"data row7 col5\" >4.000000</td>\n",
       "      <td id=\"T_771b9_row7_col6\" class=\"data row7 col6\" >7.900000</td>\n",
       "      <td id=\"T_771b9_row7_col7\" class=\"data row7 col7\" >0</td>\n",
       "      <td id=\"T_771b9_row7_col8\" class=\"data row7 col8\" >0.680000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_771b9_level0_row8\" class=\"row_heading level0 row8\" >8</th>\n",
       "      <td id=\"T_771b9_row8_col0\" class=\"data row8 col0\" >9</td>\n",
       "      <td id=\"T_771b9_row8_col1\" class=\"data row8 col1\" >302</td>\n",
       "      <td id=\"T_771b9_row8_col2\" class=\"data row8 col2\" >102</td>\n",
       "      <td id=\"T_771b9_row8_col3\" class=\"data row8 col3\" >1</td>\n",
       "      <td id=\"T_771b9_row8_col4\" class=\"data row8 col4\" >2.000000</td>\n",
       "      <td id=\"T_771b9_row8_col5\" class=\"data row8 col5\" >1.500000</td>\n",
       "      <td id=\"T_771b9_row8_col6\" class=\"data row8 col6\" >8.000000</td>\n",
       "      <td id=\"T_771b9_row8_col7\" class=\"data row8 col7\" >0</td>\n",
       "      <td id=\"T_771b9_row8_col8\" class=\"data row8 col8\" >0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_771b9_level0_row9\" class=\"row_heading level0 row9\" >9</th>\n",
       "      <td id=\"T_771b9_row9_col0\" class=\"data row9 col0\" >10</td>\n",
       "      <td id=\"T_771b9_row9_col1\" class=\"data row9 col1\" >323</td>\n",
       "      <td id=\"T_771b9_row9_col2\" class=\"data row9 col2\" >108</td>\n",
       "      <td id=\"T_771b9_row9_col3\" class=\"data row9 col3\" >3</td>\n",
       "      <td id=\"T_771b9_row9_col4\" class=\"data row9 col4\" >3.500000</td>\n",
       "      <td id=\"T_771b9_row9_col5\" class=\"data row9 col5\" >3.000000</td>\n",
       "      <td id=\"T_771b9_row9_col6\" class=\"data row9 col6\" >8.600000</td>\n",
       "      <td id=\"T_771b9_row9_col7\" class=\"data row9 col7\" >0</td>\n",
       "      <td id=\"T_771b9_row9_col8\" class=\"data row9 col8\" >0.450000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7f7c93072d10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cleaned_df.info()\n",
    "display(cleaned_df.describe().style.set_table_styles(table_styles['cerulean_palette']).set_caption(\"Jamboree data\"))\n",
    "display(cleaned_df.head(10).style.set_table_styles(table_styles['cerulean_palette']).set_caption(\"Jamboree data after cleaning\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Test data splitting \n",
    "Separate the test data after before visualisation to avoid data snooping bias\n",
    "* Note to set random seed so that train and test won't get mixed in subsequent runs\n",
    "* Use a hash function in case if online training and new data is added. This is to ensure that old test data is not mixed with new training data\n",
    "* If your dataset is not large enough (especially relative to the number of attributes), then you run the risk of introducing a significant sampling bias while doing random sampling => use stratified sampling\n",
    "\t* Find the primary predictor from business\n",
    "\t* Split the data into that predictor categories (strata)  such that each strata is not too small\n",
    "\t* Split using sklearn "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X = cleaned_df.drop('chance_of_admit', axis=1)\n",
    "y = cleaned_df['chance_of_admit']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify= cleaned_df['university_rating'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline I : Stationary Pipeline\n",
    "* Those which can be applied to both predictor and target variable (separately)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Handling Outliers and Missing Values\n",
    "- Handling Outliers:  (to be done before missing value imputation to remove effect of outliers)\n",
    "\t- Remove outliers: In some cases, it may be appropriate to simply remove the observations that contain outliers. This can be particularly useful if you have a large number of observations and the outliers are not true representatives of the underlying population.\n",
    "\t- Transform outliers: The impact of outliers can be reduced or eliminated by transforming the feature. For example, a log transformation of a feature can reduce the skewness in the data, reducing the impact of outliers.\n",
    "\t- Impute outliers: In this case, outliers are simply considered as missing values. You can employ various imputation techniques for missing values, such as mean, median, mode, nearest neighbor, etc., to impute the values for outliers.\n",
    "\t- Use robust statistical methods: Some of the statistical methods are less sensitive to outliers and can provide more reliable results when outliers are present in the data. For example, we can use median and IQR for the statistical analysis as they are not affected by the outlier’s presence. This way we can minimize the impact of outliers in statistical analysis.\n",
    "\t- Use discretization or binning : converting numerical variables to categorical form can result in some loss of information, as the precise numerical values within each bin are no longer distinguished thus quality will be reduced thus accuracy of ML model but good for EDA\n",
    "\t  Use Freedman-Diaconis rule to get bin size (same is used by sns when you give bins=n) [numpy implementation](https://medium.com/@maxmarkovvision/optimal-number-of-bins-for-histograms-3d7c48086fde) \n",
    "    - Example: Identify and remove salary values that are more than 3 standard deviations away from the mean, but are these all pertaining to a specific role ? then it might be a feature for that \n",
    "- Handling Missing Data: Impute using mean or mode with or without grouping by other categories, and check for patterns in missingness.\n",
    "    - Example: Impute missing values in the 'age' column with the mean age grouped by 'gender'.\n",
    "    - Check category wise missing \n",
    "\t- MCAR, MAR, MNAR\n",
    "\t- For sting data type there could be entries like ' ' or 'unknown' like that which are essentially like a missing value (not an issue for category because we can catch it when we take value_counts())\n",
    "\t- Use sklearn methods like `SimpleImputer`, `KNNImputer` or `IterativeImputer` (doesn't this cause multicolinearity ?)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Transformation\n",
    "* Add promising transformations of features (e.g., log(x), sqrt(x), x^2, etc.) to remove the skewness \n",
    "* Feature scaling: standardize or normalize features.\n",
    "\t* use tranformers to do this with pipeline \n",
    "* Encode categorical data \n",
    "\t* `OrdinalEncoder()` => only if categories are ordinal like bad, avg, good\n",
    "\t* `OneHotEncoder()` => if not ordinal or if distance between ordinal categories are not same and outputs as SciPy sparse matrix,  (stores cat details in feature_names_in_ attribute)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline II : Feature Engineering Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering\n",
    "- Generating New Features: Create new features like the difference between purchase date and delivery date or segment numerical data into categorical bins. Our model should not have confounding variables issue\n",
    "  > this is an iterative process: once you get a prototype up and running, you can analyze its output to gain more insights and come back to this exploration step\n",
    "\t- Drop the attributes that provide no useful information for the task. \n",
    "    - New Feature by changing reference point \n",
    "\t    - Example: Create a new feature 'delivery_time' as the difference between 'delivery_date' and 'purchase_date'.\n",
    "    - Granulize and de granulize values \n",
    "        - Granulizing\n",
    "            - GDP per captia from GDP\n",
    "            - Find average consumption of each district from state consumption by taking ratio of state consumption / no. of districts \n",
    "        - De graulize\n",
    "            -  Sum up the value of each district to get the state consumption => for better understanding divide it by no of state to get average district spending for that state\n",
    "            - AOV for each customer group => we can know how much a customer is purchasing on average \n",
    "    - Combine 2 variables to capture their relationship \n",
    "\t    - See what happens to x\\*y or x/y etc with this https://www.desmos.com/\n",
    "\t    - if income is x and y is income then y/x => lesser age people with high income will be rewarded more  <= inverse relation with age and direct with income \n",
    "\t    - if x\\*y => younger people with more income or older people with less income will be more rewarded (x and y have direct relationship)\n",
    "\t- Discretize continuous features\n",
    "\t\t- Categorize to bins such that no bin has very less values\n",
    "\t- Decompose features (e.g., categorical, date/time, etc.).\n",
    "\t\t- Example: the day of the week, month, or season to capture patterns\n",
    "\t- Check for categories with very low counts (which may need to be grouped or removed).\n",
    "        - Example: Group infrequent 'product_category' values into an 'Other' category, collapse 'single', 'unmarried', 's'\n",
    "    - [ ]  Data Leakage: Ensure that future information is not inadvertently included in the training data (especially relevant for time series or predictive modeling).\n",
    "    - Example: In a predictive model for stock prices, ensure that future stock prices are not included in the training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualisation and Exploration\n",
    "* convert pipeline output to pd.DataFrame"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
