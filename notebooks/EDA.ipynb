{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Intro to the Dataset and the Aim\n",
    "\\<img src=\"/jamboree_logo.png\" alt=\"jamboree logo banner\" style=\"width: 800px;\"/>\n",
    "\n",
    "Jamboree has helped thousands of students like you make it to top colleges abroad. Be it GMAT, GRE or SAT, their unique problem-solving methods ensure maximum scores with minimum effort.\n",
    "\n",
    "Jamboree team wants to know what factors are important for a students success in getting into an IVY league college. They also want to see if we can make a predictive model to predict the chance of admission to IVY league college using the given features.\n",
    "\n",
    "**Dataset**\n",
    "\n",
    "This dataset contains the details of 500 students who have applied for admission to IVY league college along with their success rate.\n",
    "\n",
    "Summary of sanitized data:\n",
    "| Column              | Description         | Data Type  |\n",
    "|---------------------|---------------------|------------|\n",
    "| `serial_no`         | Unique row ID       | `int64`    |\n",
    "| `gre_score`         | out of 340          | `int64`    |\n",
    "| `toefl_score`       | out of 120          | `int64`    |\n",
    "| `university_rating` | out of 5            | `category` |\n",
    "| `sop`               | out of 5            | `category` |\n",
    "| `lor`               | out of 5            | `category` |\n",
    "| `cgpa`              | out of 10           | `category` |\n",
    "| `research`          | either 0 or 1       | `category` |\n",
    "| `chance_of_admit`   | ranging from 0 to 1 | `float64`  |\n",
    "\n",
    "Additional feature engineered columns:\n",
    "\n",
    "| Column | Description | Expected Data Type |\n",
    "|--------|-------------|--------------------|\n",
    "| `GRE`  | out of 340  | `int64`            |\n",
    "\n",
    "**Aim:** \n",
    "1. To anlyze what factors are important for a students success in getting into an IVY league college.\n",
    "2. To make a predictive model to predict the chance of admission (`chance_of_admit`) to IVY league college using the given features.\n",
    "\n",
    "**Methods and Techniques used:** EDA, feature engineering, modeling using sklearn pipelines, hyperparameter tuning\n",
    "\n",
    "**Measure of Performance and Minimum Threshold to reach the business objective** : RMSE of 5% or less\n",
    "\n",
    "**Assumptions**\n",
    "1. This fairly small dataset (500 entries) is representative of the real world population.\n",
    "2. The data is stable and does not change over time. Thus model assumed to not decay. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Library Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scientific libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Logging\n",
    "import logging\n",
    "\n",
    "# Visual libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Helper libraries\n",
    "import urllib.request\n",
    "from tqdm.notebook import tqdm, trange # Progress bar\n",
    "#from colorama import Fore, Back, Style # coloured text in output\n",
    "import warnings \n",
    "#warnings.filterwarnings('ignore') # ignore all warkings\n",
    "\n",
    "# Visual setup\n",
    "%config InlineBackend.figure_format = 'retina' # sets the figure format to 'retina' for high-resolution displays.\n",
    "\n",
    "# Pandas options\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = 'all' # display all interaction \n",
    "\n",
    "# Table styles\n",
    "table_styles = {\n",
    "    'cerulean_palette': [\n",
    "        dict(selector=\"th\", props=[(\"color\", \"#FFFFFF\"), (\"background\", \"#004D80\"), (\"text-transform\", \"capitalize\")]),\n",
    "        dict(selector=\"td\", props=[(\"color\", \"#333333\")]),\n",
    "        dict(selector=\"table\", props=[(\"font-family\", 'Arial'), (\"border-collapse\", \"collapse\")]),\n",
    "        dict(selector='tr:nth-child(even)', props=[('background', '#D3EEFF')]),\n",
    "        dict(selector='tr:nth-child(odd)', props=[('background', '#FFFFFF')]),\n",
    "        dict(selector=\"th\", props=[(\"border\", \"1px solid #0070BA\")]),\n",
    "        dict(selector=\"td\", props=[(\"border\", \"1px solid #0070BA\")]),\n",
    "        dict(selector=\"tr:hover\", props=[(\"background\", \"#80D0FF\")]),\n",
    "        dict(selector=\"tr\", props=[(\"transition\", \"background 0.5s ease\")]),\n",
    "        dict(selector=\"th:hover\", props=[(\"font-size\", \"1.07rem\")]),\n",
    "        dict(selector=\"th\", props=[(\"transition\", \"font-size 0.5s ease-in-out\")]),\n",
    "        dict(selector=\"td:hover\", props=[('font-size', '1.07rem'),('font-weight', 'bold')]),\n",
    "        dict(selector=\"td\", props=[(\"transition\", \"font-size 0.5s ease-in-out\")])\n",
    "    ]\n",
    "}\n",
    "\n",
    "#from rich import print # color from print statement \n",
    "# Seed value for numpy.random => makes notebooks stable across runs\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Collection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataIngestor:\n",
    "    \"\"\"\"\n",
    "    A class to handle downloading data and loading it into a pandas dataframe along with basic sanity options\n",
    "    \"\"\"\n",
    "    def __init__(self, file_path : str = '../data/raw', url : str = None, output_path : str = 'data/processed'):\n",
    "        if (url is None and file_path == 'data/raw') or (url is not None and file_path != '../data/raw'):\n",
    "            raise ValueError('Either url or file_path must/only be specified')\n",
    "        self.file_path = f\"{file_path}\"+f\"/{url.split('/')[-1]}\" if url is not None else file_path # save non default user specified path\n",
    "        self.url = url\n",
    "        self.output_path = output_path\n",
    "    \n",
    "    def download_data(self) -> None:\n",
    "        logging.info(f'Downloading data from {self.url}')\n",
    "        urllib.request.urlretrieve(self.url, self.file_path)\n",
    "\n",
    "    def load_data(self) -> pd.DataFrame:\n",
    "        logging.info(f'Ingesting data from {self.file_path}')\n",
    "        #TODO add csv check\n",
    "        return pd.read_csv(self.file_path)\n",
    "    \n",
    "    def save_data(self, df : pd.DataFrame) -> None:\n",
    "        logging.info(f'Saving data to {self.output_path}')\n",
    "        df.to_csv(self.output_path)\n",
    "        \n",
    "    def sanitize(self, df : pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Rename columns to snake case and strip whitespace\n",
    "        \"\"\"\n",
    "        return df.rename(lambda x: x.lower().strip().replace(' ', '_'),axis='columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_url = DataIngestor(url = 'https://d2beiqkhq929f0.cloudfront.net/public_assets/assets/000/001/839/original/Jamboree_Admission.csv')\n",
    "\n",
    "data_url.download_data()\n",
    "df = data_url.sanitize(data_url.load_data())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>serial_no.</th>\n",
       "      <th>gre_score</th>\n",
       "      <th>toefl_score</th>\n",
       "      <th>university_rating</th>\n",
       "      <th>sop</th>\n",
       "      <th>lor</th>\n",
       "      <th>cgpa</th>\n",
       "      <th>research</th>\n",
       "      <th>chance_of_admit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>337</td>\n",
       "      <td>118</td>\n",
       "      <td>4</td>\n",
       "      <td>4.5</td>\n",
       "      <td>4.5</td>\n",
       "      <td>9.65</td>\n",
       "      <td>1</td>\n",
       "      <td>0.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>324</td>\n",
       "      <td>107</td>\n",
       "      <td>4</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>8.87</td>\n",
       "      <td>1</td>\n",
       "      <td>0.76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>316</td>\n",
       "      <td>104</td>\n",
       "      <td>3</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>8.00</td>\n",
       "      <td>1</td>\n",
       "      <td>0.72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>322</td>\n",
       "      <td>110</td>\n",
       "      <td>3</td>\n",
       "      <td>3.5</td>\n",
       "      <td>2.5</td>\n",
       "      <td>8.67</td>\n",
       "      <td>1</td>\n",
       "      <td>0.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>314</td>\n",
       "      <td>103</td>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>8.21</td>\n",
       "      <td>0</td>\n",
       "      <td>0.65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>495</th>\n",
       "      <td>496</td>\n",
       "      <td>332</td>\n",
       "      <td>108</td>\n",
       "      <td>5</td>\n",
       "      <td>4.5</td>\n",
       "      <td>4.0</td>\n",
       "      <td>9.02</td>\n",
       "      <td>1</td>\n",
       "      <td>0.87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>496</th>\n",
       "      <td>497</td>\n",
       "      <td>337</td>\n",
       "      <td>117</td>\n",
       "      <td>5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>9.87</td>\n",
       "      <td>1</td>\n",
       "      <td>0.96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497</th>\n",
       "      <td>498</td>\n",
       "      <td>330</td>\n",
       "      <td>120</td>\n",
       "      <td>5</td>\n",
       "      <td>4.5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>9.56</td>\n",
       "      <td>1</td>\n",
       "      <td>0.93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498</th>\n",
       "      <td>499</td>\n",
       "      <td>312</td>\n",
       "      <td>103</td>\n",
       "      <td>4</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>8.43</td>\n",
       "      <td>0</td>\n",
       "      <td>0.73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499</th>\n",
       "      <td>500</td>\n",
       "      <td>327</td>\n",
       "      <td>113</td>\n",
       "      <td>4</td>\n",
       "      <td>4.5</td>\n",
       "      <td>4.5</td>\n",
       "      <td>9.04</td>\n",
       "      <td>0</td>\n",
       "      <td>0.84</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>500 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     serial_no.  gre_score  toefl_score  university_rating  sop  lor  cgpa  \\\n",
       "0             1        337          118                  4  4.5  4.5  9.65   \n",
       "1             2        324          107                  4  4.0  4.5  8.87   \n",
       "2             3        316          104                  3  3.0  3.5  8.00   \n",
       "3             4        322          110                  3  3.5  2.5  8.67   \n",
       "4             5        314          103                  2  2.0  3.0  8.21   \n",
       "..          ...        ...          ...                ...  ...  ...   ...   \n",
       "495         496        332          108                  5  4.5  4.0  9.02   \n",
       "496         497        337          117                  5  5.0  5.0  9.87   \n",
       "497         498        330          120                  5  4.5  5.0  9.56   \n",
       "498         499        312          103                  4  4.0  5.0  8.43   \n",
       "499         500        327          113                  4  4.5  4.5  9.04   \n",
       "\n",
       "     research  chance_of_admit  \n",
       "0           1             0.92  \n",
       "1           1             0.76  \n",
       "2           1             0.72  \n",
       "3           1             0.80  \n",
       "4           0             0.65  \n",
       "..        ...              ...  \n",
       "495         1             0.87  \n",
       "496         1             0.96  \n",
       "497         1             0.93  \n",
       "498         0             0.73  \n",
       "499         0             0.84  \n",
       "\n",
       "[500 rows x 9 columns]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 500 entries, 0 to 499\n",
      "Data columns (total 9 columns):\n",
      " #   Column             Non-Null Count  Dtype  \n",
      "---  ------             --------------  -----  \n",
      " 0   serial_no.         500 non-null    int64  \n",
      " 1   gre_score          500 non-null    int64  \n",
      " 2   toefl_score        500 non-null    int64  \n",
      " 3   university_rating  500 non-null    int64  \n",
      " 4   sop                500 non-null    float64\n",
      " 5   lor                500 non-null    float64\n",
      " 6   cgpa               500 non-null    float64\n",
      " 7   research           500 non-null    int64  \n",
      " 8   chance_of_admit    500 non-null    float64\n",
      "dtypes: float64(4), int64(5)\n",
      "memory usage: 35.3 KB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>serial_no.</th>\n",
       "      <th>gre_score</th>\n",
       "      <th>toefl_score</th>\n",
       "      <th>university_rating</th>\n",
       "      <th>sop</th>\n",
       "      <th>lor</th>\n",
       "      <th>cgpa</th>\n",
       "      <th>research</th>\n",
       "      <th>chance_of_admit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>500.000000</td>\n",
       "      <td>500.000000</td>\n",
       "      <td>500.000000</td>\n",
       "      <td>500.000000</td>\n",
       "      <td>500.000000</td>\n",
       "      <td>500.00000</td>\n",
       "      <td>500.000000</td>\n",
       "      <td>500.000000</td>\n",
       "      <td>500.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>250.500000</td>\n",
       "      <td>316.472000</td>\n",
       "      <td>107.192000</td>\n",
       "      <td>3.114000</td>\n",
       "      <td>3.374000</td>\n",
       "      <td>3.48400</td>\n",
       "      <td>8.576440</td>\n",
       "      <td>0.560000</td>\n",
       "      <td>0.72174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>144.481833</td>\n",
       "      <td>11.295148</td>\n",
       "      <td>6.081868</td>\n",
       "      <td>1.143512</td>\n",
       "      <td>0.991004</td>\n",
       "      <td>0.92545</td>\n",
       "      <td>0.604813</td>\n",
       "      <td>0.496884</td>\n",
       "      <td>0.14114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>290.000000</td>\n",
       "      <td>92.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>6.800000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.34000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>125.750000</td>\n",
       "      <td>308.000000</td>\n",
       "      <td>103.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.500000</td>\n",
       "      <td>3.00000</td>\n",
       "      <td>8.127500</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.63000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>250.500000</td>\n",
       "      <td>317.000000</td>\n",
       "      <td>107.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.500000</td>\n",
       "      <td>3.50000</td>\n",
       "      <td>8.560000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.72000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>375.250000</td>\n",
       "      <td>325.000000</td>\n",
       "      <td>112.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.00000</td>\n",
       "      <td>9.040000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.82000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>500.000000</td>\n",
       "      <td>340.000000</td>\n",
       "      <td>120.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.00000</td>\n",
       "      <td>9.920000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.97000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       serial_no.   gre_score  toefl_score  university_rating         sop  \\\n",
       "count  500.000000  500.000000   500.000000         500.000000  500.000000   \n",
       "mean   250.500000  316.472000   107.192000           3.114000    3.374000   \n",
       "std    144.481833   11.295148     6.081868           1.143512    0.991004   \n",
       "min      1.000000  290.000000    92.000000           1.000000    1.000000   \n",
       "25%    125.750000  308.000000   103.000000           2.000000    2.500000   \n",
       "50%    250.500000  317.000000   107.000000           3.000000    3.500000   \n",
       "75%    375.250000  325.000000   112.000000           4.000000    4.000000   \n",
       "max    500.000000  340.000000   120.000000           5.000000    5.000000   \n",
       "\n",
       "             lor        cgpa    research  chance_of_admit  \n",
       "count  500.00000  500.000000  500.000000        500.00000  \n",
       "mean     3.48400    8.576440    0.560000          0.72174  \n",
       "std      0.92545    0.604813    0.496884          0.14114  \n",
       "min      1.00000    6.800000    0.000000          0.34000  \n",
       "25%      3.00000    8.127500    0.000000          0.63000  \n",
       "50%      3.50000    8.560000    1.000000          0.72000  \n",
       "75%      4.00000    9.040000    1.000000          0.82000  \n",
       "max      5.00000    9.920000    1.000000          0.97000  "
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df\n",
    "df.info()\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Wrangling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data Integrity and Consistency**\n",
    "- [x] Uniformity Constraint: All data should be in the same unit or format like date, currency, scales.\n",
    "    - Example: Convert all date columns to a consistent format (e.g., 'YYYY-MM-DD').\n",
    "    -  Remove symbols like % or $ \n",
    "    -  Remove scaling like 10 means 10k rupees\n",
    "- [x] Data Type Constraint: Constrain each variable to a specific data type, and check for mixed data types within a column.\n",
    "    - Example: Convert a column with mixed integers and strings to a consistent data type.\n",
    "- [x] Data Range Constraints: Each variable could have a limited range like date < current date.\n",
    "    - Example: Check if all birth dates are before the current date.\n",
    "- Uniqueness Constraint: Duplicate entries should not be there either complete row or multiple values for the same primary key => take groupby and average.\n",
    "    - Example: Check for duplicate customer IDs if there are multiple different income value / credit score then average it out \n",
    "- Text Constraint: Text should be in the expected format for that variable like phone numbers and emails following patterns and fixed lengths.\n",
    "    - Example: Check if all email addresses follow the pattern `name@domain.com`.\n",
    "    - Removing special characters, trimming whitespaces, and handling inconsistent capitalizations.\n",
    "\t- Encoding Issues: Check for consistent encoding (e.g., UTF-8, ASCII) across text data.\n",
    "\t    - Example: Convert all text columns to UTF-8 encoding.\n",
    "- Categorical Constraint and order: \n",
    "    - Uncollapsed categories with the same or similar names\n",
    "    - Rename and Specify order of category data types\n",
    "    - Check for categories with very low counts (which may need to be grouped or removed).\n",
    "    - Example: Group infrequent 'product_category' values into an 'Other' category, collapse 'single', 'unmarried', 's'\n",
    "\n",
    "**Data Validation and Relationships**\n",
    "\n",
    "- Cross-Field Validation: Compare multiple variables to get cross-field validation like delivery date >= purchase date.\n",
    "    - Example: Check if the delivery date is always greater than or equal to the purchase date.\n",
    "- Data Leakage: Ensure that future information is not inadvertently included in the training data (especially relevant for time series or predictive modeling).\n",
    "    - Example: In a predictive model for stock prices, ensure that future stock prices are not included in the training data.\n",
    "- Referential Integrity: Validate foreign key relationships between tables or datasets.\n",
    "    - Example: Check if all order IDs in the orders table have a corresponding customer ID in the customers table.\n",
    "- Business Rules: Check for any specific business rules or domain-specific constraints that the data should adhere to.\n",
    "    - Example: In a retail dataset, check if the total order value is equal to the sum of item prices.\n",
    "- **Hierarchical Validation**: Validate the hierarchical relationships within the data, such as ensuring that a subcategory belongs to the correct main category.\n",
    "\t- Example: Check if all 'product_subcategory' values correctly correspond to their 'product_category' values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -ve testing\n",
    "# df.iloc[2,1] = 1000\n",
    "# df.iloc[3,4] = 1000\n",
    "# df.iloc[5,3] = 1000\n",
    "\n",
    "class DataWrangler:\n",
    "    \"\"\"\"\n",
    "    A class to handle cleaning and wrangling data\n",
    "    \"\"\"\n",
    "    def __init__(self, df : pd.DataFrame):\n",
    "        self.df = df\n",
    "        self.processed_df = pd.DataFrame()\n",
    "        self.failed_index_range_constrains = pd.Index([])\n",
    "        self.data_type_map = {}\n",
    "        \n",
    "    def clean(self) -> pd.DataFrame:\n",
    "        return self.df\n",
    "    \n",
    "    def set_data_type(self, data_type_map : dict) -> pd.DataFrame:\n",
    "        self.data_type_map = data_type_map\n",
    "        self.processed_df = self.df.astype(data_type_map)\n",
    "        return self.processed_df\n",
    "    \n",
    "    def range_constrain_check(self, constrains : dict) -> pd.Index:\n",
    "        self.range_constrains = constrains\n",
    "        mask = np.array([False]*len(self.df))\n",
    "        for col,(min_range, max_range) in self.range_constrains.items(): \n",
    "            mask += (self.df[col] < min_range) | (self.df[col] > max_range)\n",
    "        self.failed_index_range_constrains = self.df[mask].index\n",
    "        return self.failed_index_range_constrains\n",
    "    \n",
    "    \n",
    "range_constrains = {\n",
    "    'gre_score': [0, 340],\n",
    "    'toefl_score': [0, 120],\n",
    "    'university_rating': [0, 5],\n",
    "    'sop': [0, 5],\n",
    "    'lor': [0, 5],\n",
    "    'cgpa': [0, 10],\n",
    "    'research': [0, 1]\n",
    "}\n",
    "\n",
    "data_type_map = {\n",
    "    'serial_no.': 'int16',\n",
    "    'gre_score': 'int16',\n",
    "    'toefl_score': 'int16',\n",
    "    'university_rating': 'category',\n",
    "    'sop': 'category',\n",
    "    'lor': 'category',\n",
    "    'cgpa': 'float16',\n",
    "    'research': 'int16'\n",
    "}\n",
    "clean_data = DataWrangler(df)\n",
    "idx = clean_data.range_constrain_check(range_constrains)\n",
    "\n",
    "\n",
    "df = clean_data.set_data_type(data_type_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 500 entries, 0 to 499\n",
      "Data columns (total 9 columns):\n",
      " #   Column             Non-Null Count  Dtype   \n",
      "---  ------             --------------  -----   \n",
      " 0   serial_no.         500 non-null    int16   \n",
      " 1   gre_score          500 non-null    int16   \n",
      " 2   toefl_score        500 non-null    int16   \n",
      " 3   university_rating  500 non-null    category\n",
      " 4   sop                500 non-null    category\n",
      " 5   lor                500 non-null    category\n",
      " 6   cgpa               500 non-null    float16 \n",
      " 7   research           500 non-null    int16   \n",
      " 8   chance_of_admit    500 non-null    float64 \n",
      "dtypes: category(3), float16(1), float64(1), int16(4)\n",
      "memory usage: 11.3 KB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>serial_no.</th>\n",
       "      <th>gre_score</th>\n",
       "      <th>toefl_score</th>\n",
       "      <th>university_rating</th>\n",
       "      <th>sop</th>\n",
       "      <th>lor</th>\n",
       "      <th>cgpa</th>\n",
       "      <th>research</th>\n",
       "      <th>chance_of_admit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>337</td>\n",
       "      <td>118</td>\n",
       "      <td>4</td>\n",
       "      <td>4.5</td>\n",
       "      <td>4.5</td>\n",
       "      <td>9.648438</td>\n",
       "      <td>1</td>\n",
       "      <td>0.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>324</td>\n",
       "      <td>107</td>\n",
       "      <td>4</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>8.867188</td>\n",
       "      <td>1</td>\n",
       "      <td>0.76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>316</td>\n",
       "      <td>104</td>\n",
       "      <td>3</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>0.72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>322</td>\n",
       "      <td>110</td>\n",
       "      <td>3</td>\n",
       "      <td>3.5</td>\n",
       "      <td>2.5</td>\n",
       "      <td>8.671875</td>\n",
       "      <td>1</td>\n",
       "      <td>0.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>314</td>\n",
       "      <td>103</td>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>8.210938</td>\n",
       "      <td>0</td>\n",
       "      <td>0.65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>495</th>\n",
       "      <td>496</td>\n",
       "      <td>332</td>\n",
       "      <td>108</td>\n",
       "      <td>5</td>\n",
       "      <td>4.5</td>\n",
       "      <td>4.0</td>\n",
       "      <td>9.023438</td>\n",
       "      <td>1</td>\n",
       "      <td>0.87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>496</th>\n",
       "      <td>497</td>\n",
       "      <td>337</td>\n",
       "      <td>117</td>\n",
       "      <td>5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>9.867188</td>\n",
       "      <td>1</td>\n",
       "      <td>0.96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497</th>\n",
       "      <td>498</td>\n",
       "      <td>330</td>\n",
       "      <td>120</td>\n",
       "      <td>5</td>\n",
       "      <td>4.5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>9.562500</td>\n",
       "      <td>1</td>\n",
       "      <td>0.93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498</th>\n",
       "      <td>499</td>\n",
       "      <td>312</td>\n",
       "      <td>103</td>\n",
       "      <td>4</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>8.429688</td>\n",
       "      <td>0</td>\n",
       "      <td>0.73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499</th>\n",
       "      <td>500</td>\n",
       "      <td>327</td>\n",
       "      <td>113</td>\n",
       "      <td>4</td>\n",
       "      <td>4.5</td>\n",
       "      <td>4.5</td>\n",
       "      <td>9.039062</td>\n",
       "      <td>0</td>\n",
       "      <td>0.84</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>500 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     serial_no.  gre_score  toefl_score university_rating  sop  lor      cgpa  \\\n",
       "0             1        337          118                 4  4.5  4.5  9.648438   \n",
       "1             2        324          107                 4  4.0  4.5  8.867188   \n",
       "2             3        316          104                 3  3.0  3.5  8.000000   \n",
       "3             4        322          110                 3  3.5  2.5  8.671875   \n",
       "4             5        314          103                 2  2.0  3.0  8.210938   \n",
       "..          ...        ...          ...               ...  ...  ...       ...   \n",
       "495         496        332          108                 5  4.5  4.0  9.023438   \n",
       "496         497        337          117                 5  5.0  5.0  9.867188   \n",
       "497         498        330          120                 5  4.5  5.0  9.562500   \n",
       "498         499        312          103                 4  4.0  5.0  8.429688   \n",
       "499         500        327          113                 4  4.5  4.5  9.039062   \n",
       "\n",
       "     research  chance_of_admit  \n",
       "0           1             0.92  \n",
       "1           1             0.76  \n",
       "2           1             0.72  \n",
       "3           1             0.80  \n",
       "4           0             0.65  \n",
       "..        ...              ...  \n",
       "495         1             0.87  \n",
       "496         1             0.96  \n",
       "497         1             0.93  \n",
       "498         0             0.73  \n",
       "499         0             0.84  \n",
       "\n",
       "[500 rows x 9 columns]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 500 entries, 0 to 499\n",
      "Data columns (total 9 columns):\n",
      " #   Column             Non-Null Count  Dtype  \n",
      "---  ------             --------------  -----  \n",
      " 0   serial_no.         500 non-null    int64  \n",
      " 1   gre_score          500 non-null    int64  \n",
      " 2   toefl_score        500 non-null    int64  \n",
      " 3   university_rating  500 non-null    int64  \n",
      " 4   sop                500 non-null    float64\n",
      " 5   lor                500 non-null    float64\n",
      " 6   cgpa               500 non-null    float64\n",
      " 7   research           500 non-null    int64  \n",
      " 8   chance_of_admit    500 non-null    float64\n",
      "dtypes: float64(4), int64(5)\n",
      "memory usage: 35.3 KB\n"
     ]
    }
   ],
   "source": [
    "df.info()\n",
    "df\n",
    "\n",
    "clean_data.df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Asserts and validations\n",
    "`assert bool`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Test data splitting \n",
    "Separate the test data after before visualisation to avoid data snooping bias\n",
    "* Note to set random seed so that train and test won't get mixed in subsequent runs\n",
    "* Use a hash function in case if online training and new data is added. This is to ensure that old test data is not mixed with new training data\n",
    "* If your dataset is not large enough (especially relative to the number of attributes), then you run the risk of introducing a significant sampling bias while doing random sampling => use stratified sampling\n",
    "\t* Find the primary predictor from business\n",
    "\t* Split the data into that predictor categories (strata)  such that each strata is not too small\n",
    "\t* Split using sklearn "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline I : Stationary Pipeline\n",
    "* Those which can be applied to both predictor and target variable (separately)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Handling Outliers and Missing Values\n",
    "- Handling Outliers:  (to be done before missing value imputation to remove effect of outliers)\n",
    "\t- Remove outliers: In some cases, it may be appropriate to simply remove the observations that contain outliers. This can be particularly useful if you have a large number of observations and the outliers are not true representatives of the underlying population.\n",
    "\t- Transform outliers: The impact of outliers can be reduced or eliminated by transforming the feature. For example, a log transformation of a feature can reduce the skewness in the data, reducing the impact of outliers.\n",
    "\t- Impute outliers: In this case, outliers are simply considered as missing values. You can employ various imputation techniques for missing values, such as mean, median, mode, nearest neighbor, etc., to impute the values for outliers.\n",
    "\t- Use robust statistical methods: Some of the statistical methods are less sensitive to outliers and can provide more reliable results when outliers are present in the data. For example, we can use median and IQR for the statistical analysis as they are not affected by the outlier’s presence. This way we can minimize the impact of outliers in statistical analysis.\n",
    "\t- Use discretization or binning : converting numerical variables to categorical form can result in some loss of information, as the precise numerical values within each bin are no longer distinguished thus quality will be reduced thus accuracy of ML model but good for EDA\n",
    "\t  Use Freedman-Diaconis rule to get bin size (same is used by sns when you give bins=n) [numpy implementation](https://medium.com/@maxmarkovvision/optimal-number-of-bins-for-histograms-3d7c48086fde) \n",
    "    - Example: Identify and remove salary values that are more than 3 standard deviations away from the mean, but are these all pertaining to a specific role ? then it might be a feature for that \n",
    "- Handling Missing Data: Impute using mean or mode with or without grouping by other categories, and check for patterns in missingness.\n",
    "    - Example: Impute missing values in the 'age' column with the mean age grouped by 'gender'.\n",
    "    - Check category wise missing \n",
    "\t- MCAR, MAR, MNAR\n",
    "\t- For sting data type there could be entries like ' ' or 'unknown' like that which are essentially like a missing value (not an issue for category because we can catch it when we take value_counts())\n",
    "\t- Use sklearn methods like `SimpleImputer`, `KNNImputer` or `IterativeImputer` (doesn't this cause multicolinearity ?)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Transformation\n",
    "* Add promising transformations of features (e.g., log(x), sqrt(x), x^2, etc.) to remove the skewness \n",
    "* Feature scaling: standardize or normalize features.\n",
    "\t* use tranformers to do this with pipeline \n",
    "* Encode categorical data \n",
    "\t* `OrdinalEncoder()` => only if categories are ordinal like bad, avg, good\n",
    "\t* `OneHotEncoder()` => if not ordinal or if distance between ordinal categories are not same and outputs as SciPy sparse matrix,  (stores cat details in feature_names_in_ attribute)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline II : Feature Engineering Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering\n",
    "- Generating New Features: Create new features like the difference between purchase date and delivery date or segment numerical data into categorical bins. Our model should not have confounding variables issue\n",
    "  > this is an iterative process: once you get a prototype up and running, you can analyze its output to gain more insights and come back to this exploration step\n",
    "\t- Drop the attributes that provide no useful information for the task. \n",
    "    - New Feature by changing reference point \n",
    "\t    - Example: Create a new feature 'delivery_time' as the difference between 'delivery_date' and 'purchase_date'.\n",
    "    - Granulize and de granulize values \n",
    "        - Granulizing\n",
    "            - GDP per captia from GDP\n",
    "            - Find average consumption of each district from state consumption by taking ratio of state consumption / no. of districts \n",
    "        - De graulize\n",
    "            -  Sum up the value of each district to get the state consumption => for better understanding divide it by no of state to get average district spending for that state\n",
    "            - AOV for each customer group => we can know how much a customer is purchasing on average \n",
    "    - Combine 2 variables to capture their relationship \n",
    "\t    - See what happens to x\\*y or x/y etc with this https://www.desmos.com/\n",
    "\t    - if income is x and y is income then y/x => lesser age people with high income will be rewarded more  <= inverse relation with age and direct with income \n",
    "\t    - if x\\*y => younger people with more income or older people with less income will be more rewarded (x and y have direct relationship)\n",
    "\t- Discretize continuous features\n",
    "\t\t- Categorize to bins such that no bin has very less values\n",
    "\t- Decompose features (e.g., categorical, date/time, etc.).\n",
    "\t\t- Example: the day of the week, month, or season to capture patterns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualisation and Exploration\n",
    "* convert pipeline output to pd.DataFrame"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
